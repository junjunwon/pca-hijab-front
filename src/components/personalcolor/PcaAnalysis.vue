<template>
  <div class="container  step4">
    <!-- ìƒë‹¨ ë¡œë”©ë°” -->
    <LoadingBar :isLoading="isLoading" />
    <div class="card">
      <!-- ì¤‘ê°„ì˜ ì§ˆë¬¸ê³¼ ë¹„ë””ì˜¤ ë…¹í™” -->
      <div class="content">
        <div class="question-area">
          <p class="nav"><span class="n">{{ "test" }}</span></p>
          <p class="title">{{ guideSentence }}
            <span @click="generateSpeech" class="speaker -on"></span>
          </p>
          <!-- <p v-else>ë¡œë”© ì¤‘...</p> -->
        </div>

        <div class="box">
          <div class="video-area">
            <video autoplay muted ref="video"></video>
            <img v-if=imageSrc :src="imageSrc" alt="Face Image" />
          </div>
        </div>
        <canvas ref="captureCanvas" style="display: none;"></canvas>
      </div>
    </div>
    <!-- í•˜ë‹¨ ë²„íŠ¼ -->
    <div class="">
      <button @click="detectFace">Detect Face</button>
      <button v-if=imageSrc @click="analyze">Analyze Personal Color</button>
    </div>
  </div>
</template>

<script>
// import axios from '../../plugins/axios';
import * as faceapi from "face-api.js";
// import ColorThief from "colorthief";
import { mapState, mapMutations, mapActions, mapGetters } from 'vuex';
import LoadingBar from '../util/LoadingBar.vue';

export default {
  components: {
    LoadingBar
  },
  data() {
    return {
      isLoading: false,
      synth: window.speechSynthesis,
      isRecording: false, // ìŒì„± ì¸ì‹ ìƒíƒœ í™•ì¸
      personalColor: null, // ë¶„ì„ ê²°ê³¼ ì €ìž¥
      image: new Image(),
      imageSrc: '',
      guideSentence: 'ðŸ“¸ ë¨¸ë¦¬ì¹´ë½ì€ ë’¤ë¡œ ë‚¨ê¸°ê³ , ë§¨ ì–¼êµ´ë¡œ ì •ë©´ì„ ë°”ë¼ë´ì£¼ì„¸ìš”.',

    };
  },
  computed: {
    ...mapState(['']),
    ...mapGetters(['']),
  },
  watch: {
  },
  mounted() {
    this.generateSpeech();
    this.loadFaceApiModels();
    this.startRec();
  },
  methods: {
    ...mapMutations(['setDetectedImage']),
    ...mapActions(['analysisImage']),
    async loadFaceApiModels() {
      await faceapi.nets.tinyFaceDetector.loadFromUri("/models");
      await faceapi.nets.faceLandmark68Net.loadFromUri("/models");
      console.log("Face API models loaded");
    },
    startRec() {
      this.$refs.video.srcObject = null;
      this.captureCamera((stream) => {
        const video = this.$refs["video"];
        video.srcObject = stream;
      });
    },
    captureCamera(callback) {
      navigator.mediaDevices
        .getUserMedia({
          video: true,
          audio: false,
        })
        .then((camera) => {
          callback(camera);
        })
        .catch((error) => {
          alert("Unable to capture your camera. Please check console logs.");
          console.error(error);
        });
    },
    async detectFace() {
      try {
        console.log("captureAndAnalyze ì‹œìž‘");

        const canvas = this.$refs.captureCanvas;
        const video = this.$refs.video;

        if (!video.videoWidth || !video.videoHeight) {
          console.error("ë¹„ë””ì˜¤ í¬ê¸° í™•ì¸ ì‹¤íŒ¨", { videoWidth: video.videoWidth, videoHeight: video.videoHeight });
          alert("ë¹„ë””ì˜¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.");
          return;
        }
        console.log("ë¹„ë””ì˜¤ í¬ê¸° í™•ì¸ ì™„ë£Œ");

        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;

        const ctx = canvas.getContext("2d");
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        console.log("ìº”ë²„ìŠ¤ì— ë¹„ë””ì˜¤ ê·¸ë¦¬ê¸° ì™„ë£Œ", ctx);

        console.log("ì–¼êµ´ ì¸ì‹ ì‹œìž‘");
        const detections = await faceapi.detectAllFaces(
          canvas,
          new faceapi.TinyFaceDetectorOptions()
        );
        console.log("ì–¼êµ´ ì¸ì‹ ê²°ê³¼:", detections);

        if (!detections || detections.length === 0) {
          console.warn("ì–¼êµ´ì„ ì¸ì‹í•˜ì§€ ëª»í•¨");
          alert("ì–¼êµ´ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì–¼êµ´ì„ í™”ë©´ ì¤‘ì•™ì— ë§žì¶”ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.");
          return;
        }

        const faceBox = {
          x: detections[0].box._x,
          y: detections[0].box._y,
          width: detections[0].box._width,
          height: detections[0].box._height,
        };
        console.log("ì²« ë²ˆì§¸ ì–¼êµ´ ë°•ìŠ¤:", faceBox);

        const faceCanvas = this.extractFace(canvas, faceBox);
        console.log("faceCanvas í¬ê¸°:", faceCanvas.width, faceCanvas.height);

        // faceCanvasë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        this.imageSrc = faceCanvas.toDataURL();
        // faceCanvasë¥¼ Blobìœ¼ë¡œ ë³€í™˜
        faceCanvas.toBlob((blob) => {
          this.setDetectedImage(blob);
        })
      } catch (error) {
        console.error("ì „ì²´ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì˜¤ë¥˜ ë°œìƒ", error);
        alert("ë¶„ì„ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.");
      }
    },
    async analyze() {
      //TODO: debounce ì ìš© í•„ìš”
      const resultPca = await this.analysisImage();
      console.log(resultPca);
      //loadingë°”ì— ì•„ëž˜ ë¬¸êµ¬ ì¶”ê°€í•˜ë©´ ì¢‹ì„ë“¯?
      // 'ìž ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...\n' +
      //                       'AIê°€ ë‹¹ì‹ ì˜ í¼ìŠ¤ë„ ì»¬ëŸ¬ë¥¼ ë¶„ì„í•˜ì—¬ \n' +
      //                       'ì–´ìš¸ë¦¬ëŠ” ížˆìž¡ ì»¬ëŸ¬ë¥¼ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤ âœ¨
      alert(`í¼ìŠ¤ë„ ì»¬ëŸ¬ ë¶„ì„ ê²°ê³¼...${resultPca.data.tone} (TODO: ë³„ë„ ê²°ê³¼ íŽ˜ì´ì§€ êµ¬ì„± í•„ìš”)`);
    },
    extractFace(canvas, faceBox) {
      try {
        console.log("extractFace ì‹œìž‘:", faceBox);

        const faceCanvas = document.createElement("canvas");
        faceCanvas.width = faceBox.width;
        faceCanvas.height = faceBox.height;

        const faceCtx = faceCanvas.getContext("2d");
        faceCtx.drawImage(
          canvas,
          faceBox.x,
          faceBox.y,
          faceBox.width,
          faceBox.height,
          0,
          0,
          faceBox.width,
          faceBox.height
        );

        console.log("extractFace ì™„ë£Œ");
        return faceCanvas;
      } catch (extractError) {
        console.error("extractFace ì¤‘ ì˜¤ë¥˜ ë°œìƒ", extractError);
        throw extractError; // ìƒìœ„ë¡œ ì—ëŸ¬ ì „ë‹¬
      }
    },
    generateSpeech() {
      this.textToSpeech(this.guideSentence);
    },
    textToSpeech(text) {
      const utterance = new SpeechSynthesisUtterance(text);
      this.synth.speak(utterance);
    },
    analyzePersonalColor([r, g, b]) {
      console.log("analyzePersonalColor ì‹œìž‘:", { r, g, b });

      if (r > g && r > b) return "Warm (Spring/Autumn)";
      if (b > r && b > g) return "Cool (Winter/Summer)";
      return "Neutral";
    },
  },
};
</script>
