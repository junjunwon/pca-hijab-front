<template>
  <div class="container  step4">
    <!-- ìƒë‹¨ ë¡œë”©ë°” -->
    <LoadingBar :isLoading="isLoading" />
    <div class="card">
      <!-- ì¤‘ê°„ì˜ ì§ˆë¬¸ê³¼ ë¹„ë””ì˜¤ ë…¹í™” -->
      <div class="content">
        <div class="question-area">
          <p class="nav"><span class="n">{{ "ğŸ“¸" }}</span></p>
          <p class="title">{{ guideSentence }}
            <span @click="generateSpeech" class="speaker -on"></span>
          </p>
          <!-- <p v-else>ë¡œë”© ì¤‘...</p> -->
        </div>

        <div class="box">
          <div class="video-area">
            <video autoplay muted ref="video"></video>
            <img v-if=imageSrc :src="imageSrc" alt="Face Image" />
          </div>
        </div>
        <canvas ref="captureCanvas" style="display: none;"></canvas>
      </div>
    </div>
    <!-- í•˜ë‹¨ ë²„íŠ¼ -->
    <div class="">
      <div>
        <input multiple @change="imageUpload()" ref="images" type="file" />
      </div>
      <div>
        <button @click="detectFace">Detect Face</button>
        <button @click="analyze">Analyze Personal Color</button>
      </div>
    </div>
  </div>
</template>

<script>
// import axios from '../../plugins/axios';
import * as faceapi from "face-api.js";
// import ColorThief from "colorthief";
import { mapState, mapMutations, mapActions, mapGetters } from 'vuex';
import LoadingBar from '../util/LoadingBar.vue';

export default {
  components: {
    LoadingBar
  },
  data() {
    return {
      isLoading: false,
      synth: window.speechSynthesis,
      isRecording: false, // ìŒì„± ì¸ì‹ ìƒíƒœ í™•ì¸
      personalColor: null, // ë¶„ì„ ê²°ê³¼ ì €ì¥
      image: new Image(),
      imageSrc: '',
      guideSentence: 'ë¨¸ë¦¬ì¹´ë½ì€ ë’¤ë¡œ ë„˜ê¸°ê³ , ë§¨ ì–¼êµ´ë¡œ ì •ë©´ì„ ë°”ë¼ë´ì£¼ì„¸ìš”.',
      file: '',

    };
  },
  computed: {
    ...mapState(['']),
    ...mapGetters(['']),
  },
  watch: {
  },
  mounted() {
    this.generateSpeech();
    this.loadFaceApiModels();
    this.startRec();
  },
  methods: {
    ...mapMutations(['setDetectedImage', 'setPersonalColor', 'setDetectedImageSrc']),
    ...mapActions(['analysisImage']),
    async loadFaceApiModels() {
      await faceapi.nets.tinyFaceDetector.loadFromUri("/models");
      await faceapi.nets.faceLandmark68Net.loadFromUri("/models");
      console.log("Face API models loaded");
    },
    startRec() {
      this.$refs.video.srcObject = null;
      this.captureCamera((stream) => {
        const video = this.$refs["video"];
        video.srcObject = stream;
      });
    },
    captureCamera(callback) {
      navigator.mediaDevices
        .getUserMedia({
          video: true,
          audio: false,
        })
        .then((camera) => {
          callback(camera);
        })
        .catch((error) => {
          alert("Unable to capture your camera. Please check console logs.");
          console.error(error);
        });
    },
    async detectFace() {
      try {
        console.log("captureAndAnalyze ì‹œì‘");

        const canvas = this.$refs.captureCanvas;
        const video = this.$refs.video;

        if (!video.videoWidth || !video.videoHeight) {
          console.error("ë¹„ë””ì˜¤ í¬ê¸° í™•ì¸ ì‹¤íŒ¨", { videoWidth: video.videoWidth, videoHeight: video.videoHeight });
          alert("ë¹„ë””ì˜¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.");
          return;
        }
        console.log("ë¹„ë””ì˜¤ í¬ê¸° í™•ì¸ ì™„ë£Œ");

        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;

        const ctx = canvas.getContext("2d");
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        console.log("ìº”ë²„ìŠ¤ì— ë¹„ë””ì˜¤ ê·¸ë¦¬ê¸° ì™„ë£Œ", ctx);

        console.log("ì–¼êµ´ ì¸ì‹ ì‹œì‘");
        const detections = await faceapi.detectAllFaces(
          canvas,
          new faceapi.TinyFaceDetectorOptions()
        );
        console.log("ì–¼êµ´ ì¸ì‹ ê²°ê³¼:", detections);

        if (!detections || detections.length === 0) {
          console.warn("ì–¼êµ´ì„ ì¸ì‹í•˜ì§€ ëª»í•¨");
          alert("ì–¼êµ´ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì–¼êµ´ì„ í™”ë©´ ì¤‘ì•™ì— ë§ì¶”ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.");
          return;
        }

        const faceBox = {
          x: detections[0].box._x,
          y: detections[0].box._y,
          width: detections[0].box._width,
          height: detections[0].box._height,
        };
        console.log("ì²« ë²ˆì§¸ ì–¼êµ´ ë°•ìŠ¤:", faceBox);

        const faceCanvas = this.extractFace(canvas, faceBox);
        console.log("faceCanvas í¬ê¸°:", faceCanvas.width, faceCanvas.height);

        // faceCanvasë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        this.imageSrc = faceCanvas.toDataURL();
        this.setDetectedImageSrc(this.imageSrc);
        // faceCanvasë¥¼ Blobìœ¼ë¡œ ë³€í™˜
        faceCanvas.toBlob((blob) => {
          this.setDetectedImage(blob);
        })
      } catch (error) {
        console.error("ì „ì²´ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì˜¤ë¥˜ ë°œìƒ", error);
        alert("ë¶„ì„ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.");
      }
    },
    async analyze() {
      //TODO: debounce ì ìš© í•„ìš”
      const resultPersonalColor = await this.analysisImage();
      console.log(resultPersonalColor);
      this.setPersonalColor(resultPersonalColor.data);
      this.$router.push({ name: 'PcaResult' });
    },
    extractFace(canvas, faceBox) {
      try {
        console.log("extractFace ì‹œì‘:", faceBox);

        const faceCanvas = document.createElement("canvas");
        faceCanvas.width = faceBox.width;
        faceCanvas.height = faceBox.height;

        const faceCtx = faceCanvas.getContext("2d");
        faceCtx.drawImage(
          canvas,
          faceBox.x,
          faceBox.y,
          faceBox.width,
          faceBox.height,
          0,
          0,
          faceBox.width,
          faceBox.height
        );

        console.log("extractFace ì™„ë£Œ");
        return faceCanvas;
      } catch (extractError) {
        console.error("extractFace ì¤‘ ì˜¤ë¥˜ ë°œìƒ", extractError);
        throw extractError; // ìƒìœ„ë¡œ ì—ëŸ¬ ì „ë‹¬
      }
    },
    generateSpeech() {
      this.textToSpeech(this.guideSentence);
    },
    textToSpeech(text) {
      const utterance = new SpeechSynthesisUtterance(text);
      this.synth.speak(utterance);
    },
    async imageUpload() {
      // refs ì†ì„±ì„ ì´ìš©í•´ input íƒœê·¸ì— ì ‘ê·¼í•¨
      this.file = this.$refs.images.files[0];
      console.log(this.file);
      this.setDetectedImageSrc(URL.createObjectURL(this.file));

      try {
        const fileBlob = await this.handleFile();
        console.log('Blob ë³€í™˜ ì™„ë£Œ:', fileBlob);
        this.setDetectedImage(fileBlob);
      } catch (error) {
        console.error('Blob ë³€í™˜ ì‹¤íŒ¨:', error);
      }
    },
    handleFile() {
      return new Promise((resolve, reject) => {
        if (this.file) {
          const reader = new FileReader();
          
          reader.onload = (e) => {
            try {
              // Base64 ë°ì´í„°ë¥¼ Blobìœ¼ë¡œ ë³€í™˜
              const binaryStr = atob(e.target.result.split(',')[1]);
              const len = binaryStr.length;
              const arr = new Uint8Array(len);

              for (let i = 0; i < len; i++) {
                arr[i] = binaryStr.charCodeAt(i);
              }

              const blob = new Blob([arr], { type: this.file.type });
              resolve(blob);
            } catch (error) {
              reject(error);
            }
          };

          reader.onerror = (error) => {
            reject(error);
          };

          reader.readAsDataURL(this.file);
        } else {
          reject(new Error('íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'));
        }
      });
    },
    //Deprecated
    analyzePersonalColor([r, g, b]) {
      console.log("analyzePersonalColor ì‹œì‘:", { r, g, b });

      if (r > g && r > b) return "Warm (Spring/Autumn)";
      if (b > r && b > g) return "Cool (Winter/Summer)";
      return "Neutral";
    },
  },
};
</script>
